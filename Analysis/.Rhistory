Slab = numeric(),
InHoop = numeric(),
Athlete = character())
# column name
import_names <- c("Participant",
"Direction",
"Slab",
"InHoop",
"Athlete")
# read in files
for(f in results_files){
# read in each file
d <- read.csv(paste("data/Part1/", f, sep = ""), header = T)
# name columns
colnames(d) <- import_names
# add to empty data frame
df_part1 <- rbind(df_part1, d)
}
# tidy
rm(results_files, import_names, f, d)
#### sort new columns ####
# get accuracy value
df_part1$Trials <- 12
df_part1$Accuracy <- df_part1$InHoop / df_part1$Trials
# make Participant a Factor
df_part1$Participant <- as.factor(df_part1$Participant)
# make participant a number to match with model...
df_part1$Participant_num <- rep(c(1:max(unique(as.numeric(df_part1$Participant)))), each = length(unique(df_part1$Slab)))
#### run GLM ####
m <- glm(data = df_part1, Accuracy~Slab:Participant,
family = binomial)
df_part1$p <- predict(m, type = "response")
# save this
save(df_part1, file = "scratch/df_part1")
#### make plot ####
plt_part1 <- df_part1 %>%
ggplot(aes(Slab, Accuracy)) +
geom_point() +
theme_bw() +
geom_smooth(colour = "blue", method = glm,
method.args = list(family = "binomial"),
aes(y = p), fullrange = T, se = F) +
facet_wrap(~Participant) +
theme(strip.text.x = element_blank())
plt_part1
# tidy
rm(m)
# make one showing a subset
#### get slabs_to_test ####
# empty data.frame
slabs_to_test <- data.frame(Participant = character(),
Acc_level = numeric(),
Test_slab = numeric())
# acc_levels we want
acc_levels <- c(90,75,25,10)
# separations we want
separations <- c(1:30)
for(P in unique(df_part1$Participant)) {
# get subset
ss <- df_part1[df_part1$Participant == P,]
# run glm for each participant
m <- glm(data = ss,
Accuracy ~ Slab,
family = binomial)
# get predictions
p <- predict(m, data.frame(Slab = separations), type = "response")
p <- as.numeric(p)
for(a in acc_levels){
slab = which(abs(p-a/100)==min(abs(p-a/100)))
slabs_to_test <- rbind(slabs_to_test, data.frame(Participant = P,
Acc_level = a,
Slab = separations[slab]))
}
}
# tidy
rm(m, ss, a, acc_levels, p, P, separations, slab)
slabs_to_test[slabs_to_test$Participant == "36" | slabs_to_test$Participant == "37",]
setwd("F:/Uni/Github/Transfer_Paper/Non_naive/Throwing")
rm(list = ls())
#### Script to get slabs to test ####
# This script is to get the average performance for all participants
# that have participated in the throwing experiment using the normal
# sized hoops so far.
# Just need the extremes so we can sample within this randomly.
#### load libraries ####
library(tidyverse)
#### read in data ####
results_files <- dir("data/Part1/")
# Need to read in each individually since they're different formats
anca_dat <- read.table(paste("data/Part1/", results_files[1], sep = ""),
header = T)
aware_dat <- read.csv(paste("data/Part1/", results_files[2], sep = ""))
CH_dat <- read.csv(paste("data/Part1/", results_files[3], sep = ""))
TwoT_dat <- read.csv(paste("data/Part1/", results_files[4], sep = ""))
# sort colnames
anca_dat <- select(anca_dat,
Participant,
Direction,
Distance,
Accuracy)
colnames(aware_dat) <- c("Participant",
"Direction",
"Distance",
"Accuracy")
colnames(CH_dat) <- c("Participant",
"Direction",
"Distance",
"Accuracy")
colnames(TwoT_dat) <- c("Participant",
"Direction",
"Distance",
"Accuracy")
# before merging, make sure there is no overlap in participant coding
anca_dat$Participant <- paste(anca_dat$Participant, "AN", sep = "")
# bind
df <- rbind(anca_dat, aware_dat)
df <- rbind(df, CH_dat)
df <- rbind(df, TwoT_dat)
# tidy
rm(anca_dat, aware_dat, CH_dat, TwoT_dat, results_files)
# factor participants
df$Participant <- as.factor(df$Participant)
#### Get GLM ####
# need total trials
df$trials <- 12
# get Acc
df$Acc <- df$Accuracy/df$trials
# offset
e <- 0.01
df$off_set = log((1-e)/e)
# tidy
rm(e)
# Probably want to extract for each participant
# So we need a loop to get the max and minimum
# set up empty frame
slabs_to_test = data.frame(Participant=character(),
slab10 = numeric(),
slab90 = numeric())
for (x in levels(df$Participant)){
# subset data
ss = df[df$Participant==x,]
# get glm
m = glm(data=ss, Acc~Distance,
#offset=ss$off_set,
family = binomial)
# get predictions
p = predict(m, data.frame(Distance=seq(0:34)), type="response")
p = as.numeric(p)
# switch point
easy = 0.9
hard = 0.1
slab10 = which(abs(p-hard)==min(abs(p-hard)))
slab90 = which(abs(p-easy)==min(abs(p-easy)))
# add to dataset
slabs_to_test = rbind(slabs_to_test, data.frame(Participant = x,
slab10 = slab10,
slab90 = slab90))
}
# tidy
rm(ss, easy, hard, p, slab10, slab90, x)
head(slabs_to_test)
rm(list = ls())
#### Avatar - Analysis script ####
# few ideas
# modelling optimal choice vs. not
# - Define a range for each distance that would get the optimal
#   Accuracy and simply model if people are in that range for each
#   condition at the various distances?
# - Could also just do mean normalised placement again with order
#   and condition as predictors?
# - What about some more simple analysis for the final years working
#   with Alasdair?
#### Library ####
library(tidyverse)
library(rstan)
library(brms) # probably won't need this...
#### Notes ####
# max speed is max(df_deltas)/100
# for Condition, 1 = Avatar, 2 = Truck
# for Spread 1 = Randunif, 2 = Hard cutoff
# do we ever want to use RT as a predictor?
# might make sense to do this and centre it on the global average?
#### Constants ####
travel_time <- 100
#### Any Functions ####
#### Load in data ####
load("scratch/data/df_decisions")
# make model data
model_data <- df_decisions %>%
mutate(Abs_Norm_pos = abs(Placed_x/Delta))
# add in binary predictors for stan modelling
# condition
model_data$cnd_rand <- 1
model_data$cnd_rand[model_data$truck_perf == "Highly_Certain"] <- 0
# reduce down columns
model_data <- model_data %>%
select(-Condition,
-Spread,
-Initial_x,
-Speed,
-Success,
-standard) %>% # only for now
mutate(Norm_Delta = Delta/max(Delta)) %>%
filter(Abs_Norm_pos < 1.01)
# load in the estimates data
load("scratch/data/df_estimates")
#### pre-analysis ####
# first want to look at the correlation of estimates accross condition
# within in each participant
# so sort that
temp <- df_estimates %>%
filter(Estimate_Type == "Participant") %>%
group_by(Participant, truck_perf, Delta) %>%
summarise(Estimate = mean(Estimate)) %>%
spread(truck_perf, Estimate) %>%
ggplot(aes(Random_Uniform, Highly_Certain,
colour = Delta)) +
geom_point() +
geom_smooth(method = "glm",
method.args = list(family = "binomial"),
se = F) +
facet_wrap(~Participant)
temp
setwd("F:/Uni/Github/Avatar_Certainty/Analysis")
#### Avatar - Analysis script ####
# few ideas
# modelling optimal choice vs. not
# - Define a range for each distance that would get the optimal
#   Accuracy and simply model if people are in that range for each
#   condition at the various distances?
# - Could also just do mean normalised placement again with order
#   and condition as predictors?
# - What about some more simple analysis for the final years working
#   with Alasdair?
#### Library ####
library(tidyverse)
library(rstan)
library(brms) # probably won't need this...
#### Notes ####
# max speed is max(df_deltas)/100
# for Condition, 1 = Avatar, 2 = Truck
# for Spread 1 = Randunif, 2 = Hard cutoff
# do we ever want to use RT as a predictor?
# might make sense to do this and centre it on the global average?
#### Constants ####
travel_time <- 100
#### Any Functions ####
#### Load in data ####
load("scratch/data/df_decisions")
# make model data
model_data <- df_decisions %>%
mutate(Abs_Norm_pos = abs(Placed_x/Delta))
# add in binary predictors for stan modelling
# condition
model_data$cnd_rand <- 1
model_data$cnd_rand[model_data$truck_perf == "Highly_Certain"] <- 0
# reduce down columns
model_data <- model_data %>%
select(-Condition,
-Spread,
-Initial_x,
-Speed,
-Success,
-standard) %>% # only for now
mutate(Norm_Delta = Delta/max(Delta)) %>%
filter(Abs_Norm_pos < 1.01)
# load in the estimates data
load("scratch/data/df_estimates")
#### pre-analysis ####
# first want to look at the correlation of estimates accross condition
# within in each participant
# so sort that
temp <- df_estimates %>%
filter(Estimate_Type == "Participant") %>%
group_by(Participant, truck_perf, Delta) %>%
summarise(Estimate = mean(Estimate)) %>%
spread(truck_perf, Estimate) %>%
ggplot(aes(Random_Uniform, Highly_Certain,
colour = Delta)) +
geom_point() +
geom_smooth(method = "glm",
method.args = list(family = "binomial"),
se = F) +
facet_wrap(~Participant)
temp
head(df_estimates)
temp <- df_estimates %>%
filter(estimate_type == "Participant") %>%
group_by(participant, truck_perf, delta) %>%
summarise(estimate = mean(estimate)) %>%
spread(truck_perf, estimate) %>%
ggplot(aes(Random_Uniform, Highly_Certain,
colour = delta)) +
geom_point() +
geom_smooth(method = "glm",
method.args = list(family = "binomial"),
se = F) +
facet_wrap(~participant)
temp
df_estimates %>%
filter(estimate_type == "Participant") %>%
group_by(participant, truck_perf, delta) %>%
summarise(estimate = mean(estimate)) %>%
spread(truck_perf, estimate)
temp <- df_estimates %>%
filter(estimate_type == "Participant") %>%
group_by(participant, truck_perf, delta) %>%
summarise(estimate = mean(estimate)) %>%
spread(truck_perf, estimate) %>%
ggplot(aes(Constant, Variable,
colour = delta)) +
geom_point() +
geom_smooth(method = "glm",
method.args = list(family = "binomial"),
se = F) +
facet_wrap(~participant)
temp
View(temp)
p = unique(df_estimates$participant)[1]
ss <- df_estimates[df_estimates$participant == p,]
# linear model
check_cor <- glm(estimate ~ (delta + truck_perf)^2,
data = ss,
family = "binomial")
View(check_cor)
summary(check_cor)
p = "05"
ss <- df_estimates[df_estimates$participant == p,]
# linear model
check_cor <- glm(estimate ~ (delta + truck_perf)^2,
data = ss,
family = "binomial")
summary(check_cor
)
rm(list= ls())
#### Avatar - Analysis script ####
# few ideas
# modelling optimal choice vs. not
# - Define a range for each distance that would get the optimal
#   Accuracy and simply model if people are in that range for each
#   condition at the various distances?
# - Could also just do mean normalised placement again with order
#   and condition as predictors?
# - What about some more simple analysis for the final years working
#   with Alasdair?
#### Library ####
library(tidyverse)
library(rstan)
library(brms) # probably won't need this...
#### Notes ####
# max speed is max(df_deltas)/100
# for Condition, 1 = Avatar, 2 = Truck
# for Spread 1 = Randunif, 2 = Hard cutoff
# do we ever want to use RT as a predictor?
# might make sense to do this and centre it on the global average?
#### Constants ####
travel_time <- 100
#### Any Functions ####
#### Load in data ####
load("scratch/data/df_decisions")
# make model data
model_data <- df_decisions %>%
mutate(Abs_Norm_pos = abs(Placed_x/Delta))
# add in binary predictors for stan modelling
# condition
model_data$cnd_rand <- 1
model_data$cnd_rand[model_data$truck_perf == "Highly_Certain"] <- 0
# reduce down columns
model_data <- model_data %>%
select(-Condition,
-Spread,
-Initial_x,
-Speed,
-Success,
-standard) %>% # only for now
mutate(Norm_Delta = Delta/max(Delta)) %>%
filter(Abs_Norm_pos < 1.01)
# load in the estimates data
load("scratch/data/df_estimates")
#### pre-analysis ####
# first want to look at the correlation of estimates accross condition
# within in each participant
# so sort that
temp <- df_estimates %>%
filter(estimate_type == "Participant") %>%
group_by(participant, truck_perf, delta) %>%
summarise(estimate = mean(estimate)) %>%
spread(truck_perf, estimate) %>%
ggplot(aes(Constant, Variable,
colour = delta)) +
geom_point() +
geom_smooth(method = "glm",
method.args = list(family = "binomial"),
se = F) +
facet_wrap(~participant)
temp
#### Avatar - Analysis script ####
# few ideas
# modelling optimal choice vs. not
# - Define a range for each distance that would get the optimal
#   Accuracy and simply model if people are in that range for each
#   condition at the various distances?
# - Could also just do mean normalised placement again with order
#   and condition as predictors?
# - What about some more simple analysis for the final years working
#   with Alasdair?
#### Library ####
library(tidyverse)
library(rstan)
library(brms) # probably won't need this...
#### Notes ####
# max speed is max(df_deltas)/100
# for Condition, 1 = Avatar, 2 = Truck
# for Spread 1 = Randunif, 2 = Hard cutoff
# do we ever want to use RT as a predictor?
# might make sense to do this and centre it on the global average?
#### Constants ####
travel_time <- 100
#### Any Functions ####
#### Load in data ####
load("scratch/data/df_decisions")
# make model data
model_data <- df_decisions %>%
mutate(Abs_Norm_pos = abs(placed_x/delta))
# add in binary predictors for stan modelling
# condition
model_data$cnd_rand <- 1
model_data$cnd_rand[model_data$truck_perf == "Constant"] <- 0
# reduce down columns
model_data <- model_data %>%
select(-condition,
-spread,
-initial_x,
-speed,
-success,
-standard) %>% # only for now
mutate(Norm_Delta = delta/max(delta)) %>%
filter(Abs_Norm_pos < 1.01)
# load in the estimates data
load("scratch/data/df_estimates")
model_data <- model_data %>%
select(-condition,
-spread,
-initial_x,
-speed,
-success) %>% # only for now
mutate(Norm_Delta = delta/max(delta)) %>%
filter(Abs_Norm_pos < 1.01)
View(temp)
View(temp[["data"]])
model_brms_3<- brm(Abs_Norm_pos ~ (Norm_Delta + truck_perf)^2,
data = model_data,
family = "beta",
iter = 2000,
chains = 1,
cores = 1)
plot(marginal_effects(model_brms_3))
library(tidyverse)
load("scratch/data/df_decisions")
df_decisions <- as.tibble(df_decisions)
# compute mean and var for each person and condition
(df_decisions %>%
select(participant, truck_perf, delta, placed_x) %>%
mutate(norm_dist = abs(placed_x / delta)) %>%
group_by(participant, truck_perf, delta) %>%
summarise(
mean_position = mean(norm_dist),
var_dist = var(norm_dist))) -> df
plt <- ggplot(df, aes(x = delta, y = mean_position, colour = truck_perf))
plt <- plt + geom_path(aes(group = participant), alpha = 0.33)
plt <- plt + geom_smooth(se = FALSE, size = 3)
plt <- plt + scale_colour_viridis_d(end = 0.5)
plt
# now aggregate to compute mean of means and related standard error
(df %>%
group_by(truck_perf, delta) %>%
summarise(
n = n(),
position = mean(mean_position),
std_err_p = sd(mean_position)/ sqrt(n),
variance = mean(var_dist),
std_err_v = sd(var_dist)/ sqrt(n))) -> df2
position_aov <- aov(data = df2, position ~ truck_perf * delta)
summary(position_aov)
variance_aov <- aov(data = df2, position ~ truck_perf * delta)
summary(variance_aov)
# plot how mean position varies with delta and condition
plt_mean <- ggplot(df2, aes(
x = delta,
y = position,
ymin = position - 1.96 * std_err_p,
ymax = position + 1.96 * std_err_p,
colour = truck_perf))
plt_mean <- plt_mean + geom_errorbar(colour = "gray") + geom_path()
plt_mean <- plt_mean + scale_y_continuous(limits = c(0, 1), expand = c(0, 0))
plt_mean <- plt_mean + scale_x_continuous(breaks = unique(df$delta))
plt_mean <- plt_mean + theme_bw()
plt_mean
# plot how variance position varies with delta and condition
plt <- ggplot(df2, aes(
x = delta,
y = variance,
ymin = variance - 1.96 * std_err_v,
ymax = variance + 1.96 * std_err_v,
colour = truck_perf))
plt <- plt + geom_errorbar(colour = "gray") + geom_path()
plt <- plt + scale_y_continuous(expand = c(0, 0))
plt <- plt + scale_x_continuous(breaks = unique(df$delta))
plt <- plt + theme_bw()
plt
var_aov <- aov(data = df2, variance ~ delta * truck_perf)
summary(var_aov)
